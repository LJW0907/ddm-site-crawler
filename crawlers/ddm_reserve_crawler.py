# crawlers/ddm_reserve_crawler.py

import time
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import json
import os
from urllib.parse import urljoin


class DDMReserveCrawler:
    """ë™ëŒ€ë¬¸êµ¬ ì˜ˆì•½í¬í„¸ í¬ë¡¤ëŸ¬ (ì „ì²´í”„ë¡œê·¸ë¨ & ì˜¨ë¼ì¸ì ‘ìˆ˜ í†µí•©)"""

    def __init__(self):
        self.base_url = "https://www.ddm.go.kr"
        # ì‹¤ì œ ë¸Œë¼ìš°ì €ì™€ ìœ ì‚¬í•œ í—¤ë” ì •ë³´ ì¶”ê°€
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
            "Accept-Encoding": "gzip, deflate, br, zstd",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)

    def _get_soup(self, url, params=None):
        """requestsë¥¼ ì‚¬ìš©í•´ BeautifulSoup ê°ì²´ë¥¼ ë°˜í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
        try:
            response = self.session.get(url, params=params, timeout=10)
            response.raise_for_status()
            response.encoding = "utf-8"  # í•œê¸€ ì¸ì½”ë”© ëª…ì‹œ
            return BeautifulSoup(response.content, "lxml")
        except requests.exceptions.RequestException as e:
            print(f"Error fetching {url}: {e}")
            return None

    def _parse_programs(self, soup, status):
        """'ì „ì²´í”„ë¡œê·¸ë¨' í˜ì´ì§€ì˜ ëª©ë¡ì„ íŒŒì‹±"""
        programs = []
        rows = soup.select("div.program.lecture tbody.text_center tr")

        if not rows:
            print(f"     -> ì „ì²´í”„ë¡œê·¸ë¨ {status}: ë°ì´í„° ì—†ìŒ")
            return programs

        for row in rows:
            cells = row.find_all("td")
            if len(cells) < 8:
                continue

            title_tag = cells[1].find("a")
            status_tag = cells[7].find("a")

            # ì ‘ìˆ˜/êµìœ¡ ê¸°ê°„ì„ ë‘ ì¤„ë¡œ ëœ <p> íƒœê·¸ì—ì„œ ì¶”ì¶œ
            date_cell_text = cells[3].get_text(separator="|").strip()
            date_parts = [p.strip() for p in date_cell_text.split("|") if p.strip()]

            application_period = date_parts[0] if date_parts else ""
            education_period = date_parts[1] if len(date_parts) > 1 else ""

            # ì‹ ì²­í•˜ê¸° ë²„íŠ¼ì˜ onclickì—ì„œ ìƒì„¸ URL íŒŒë¼ë¯¸í„° ì¶”ì¶œ
            detail_url = ""
            if title_tag and "href" in title_tag.attrs:
                detail_url = urljoin(self.base_url, title_tag["href"])
            elif status_tag and "onclick" in status_tag.attrs:
                # onclickì—ì„œ URL íŒŒë¼ë¯¸í„° ì¶”ì¶œ ì‹œë„
                onclick = status_tag["onclick"]
                if "location.href" in onclick:
                    url_match = onclick.split("'")[1] if "'" in onclick else ""
                    if url_match:
                        detail_url = urljoin(self.base_url, url_match)

            program = {
                "title": title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ",
                "location": cells[2].text.strip(),
                "application_period": application_period,
                "education_period": education_period,
                "education_time": cells[4].get_text(separator=" ", strip=True),
                "selection_method": cells[5].text.strip(),
                "capacity_status": cells[6].get_text(separator=" ", strip=True),
                "status": status,  # ì ‘ìˆ˜ì˜ˆì •/ì ‘ìˆ˜ì¤‘ ëª…ì‹œ
                "button_text": status_tag.text.strip() if status_tag else "",
                "url": detail_url,
                "type": "ì „ì²´í”„ë¡œê·¸ë¨",
                "source": "ë™ëŒ€ë¬¸êµ¬ ì˜ˆì•½í¬í„¸",
                "crawled_at": datetime.now().isoformat(),
            }
            programs.append(program)

        print(f"     -> ì „ì²´í”„ë¡œê·¸ë¨ {status}: {len(programs)}ê°œ")
        return programs

    def _parse_online_receptions(self, soup, status):
        """'ì˜¨ë¼ì¸ì ‘ìˆ˜' í˜ì´ì§€ì˜ ëª©ë¡ì„ íŒŒì‹±"""
        receptions = []
        rows = soup.select("div.online_accept.list tbody.text_center tr")

        if not rows:
            print(f"     -> ì˜¨ë¼ì¸ì ‘ìˆ˜ {status}: ë°ì´í„° ì—†ìŒ")
            return receptions

        for row in rows:
            cells = row.find_all("td")
            if len(cells) < 8:
                continue

            title_tag = cells[1].find("a")
            status_tag = cells[7].find("a")

            # ì‹ ì²­í•˜ê¸° ë²„íŠ¼ì˜ onclickì—ì„œ ìƒì„¸ URL íŒŒë¼ë¯¸í„° ì¶”ì¶œ
            detail_url = ""
            if title_tag and "href" in title_tag.attrs:
                detail_url = urljoin(self.base_url, title_tag["href"])
            elif status_tag and "onclick" in status_tag.attrs:
                onclick = status_tag["onclick"]
                if "location.href" in onclick:
                    url_match = onclick.split("'")[1] if "'" in onclick else ""
                    if url_match:
                        detail_url = urljoin(self.base_url, url_match)

            reception = {
                "title": title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ",
                "department": cells[2].text.strip(),
                "application_period": cells[3].get_text(separator="~", strip=True),
                "selection_method": cells[4].text.strip(),
                "capacity_status": cells[5]
                .get_text(separator="/", strip=True)
                .replace("\n", ""),
                "fee": cells[6].text.strip(),
                "status": status,  # ì ‘ìˆ˜ì˜ˆì •/ì ‘ìˆ˜ì¤‘ ëª…ì‹œ
                "button_text": status_tag.text.strip() if status_tag else "",
                "url": detail_url,
                "type": "ì˜¨ë¼ì¸ì ‘ìˆ˜",
                "source": "ë™ëŒ€ë¬¸êµ¬ ì˜ˆì•½í¬í„¸",
                "crawled_at": datetime.now().isoformat(),
            }
            receptions.append(reception)

        print(f"     -> ì˜¨ë¼ì¸ì ‘ìˆ˜ {status}: {len(receptions)}ê°œ")
        return receptions

    def crawl_all(self):
        """ëª¨ë“  ì˜ˆì•½/ì ‘ìˆ˜ í”„ë¡œê·¸ë¨ì„ í¬ë¡¤ë§"""
        print("\n" + "=" * 50)
        print("   [ë™ëŒ€ë¬¸êµ¬ ì˜ˆì•½í¬í„¸] í¬ë¡¤ë§ ì‹œì‘")
        print("=" * 50 + "\n")

        all_results = []

        # 1. ì „ì²´í”„ë¡œê·¸ë¨ í¬ë¡¤ë§ (ì ‘ìˆ˜ì˜ˆì •, ì ‘ìˆ˜ì¤‘) - URL íŒŒë¼ë¯¸í„° ì¶”ê°€
        program_urls = {
            "ì ‘ìˆ˜ì˜ˆì •": "https://www.ddm.go.kr/reserve/selectDongdaemunUserCourseList.do?searchEduInstSe=&key=1529&searchEdcKey=&searchEdcRealm=&searchTime=%EC%A0%91%EC%88%98%EA%B8%B0%EA%B0%84&timeBgnde=&timeEndde=&receptionStts=TBCCPT&searchCnd=SJ&searchKrwd=",
            "ì ‘ìˆ˜ì¤‘": "https://www.ddm.go.kr/reserve/selectDongdaemunUserCourseList.do?searchEduInstSe=&key=1529&searchEdcKey=&searchEdcRealm=&searchTime=%EC%A0%91%EC%88%98%EA%B8%B0%EA%B0%84&timeBgnde=&timeEndde=&receptionStts=ACCPT&searchCnd=SJ&searchKrwd=",
        }

        print("1. [ì „ì²´í”„ë¡œê·¸ë¨] í¬ë¡¤ë§")
        for status, url in program_urls.items():
            print(f"   - {status} í˜ì´ì§€ ë¡œë”©...")
            soup = self._get_soup(url)
            if soup:
                programs = self._parse_programs(soup, status)
                all_results.extend(programs)
            time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€

        # 2. ì˜¨ë¼ì¸ì ‘ìˆ˜ í¬ë¡¤ë§ (ì ‘ìˆ˜ì˜ˆì •, ì ‘ìˆ˜ì¤‘)
        reception_urls = {
            "ì ‘ìˆ˜ì˜ˆì •": "https://www.ddm.go.kr/reserve/selectUserOnlineReceptionList.do?key=3133&searchCnd=TBCCPT",
            "ì ‘ìˆ˜ì¤‘": "https://www.ddm.go.kr/reserve/selectUserOnlineReceptionList.do?key=3133&searchCnd=ACCPT",
        }

        print("\n2. [ì˜¨ë¼ì¸ì ‘ìˆ˜] í¬ë¡¤ë§")
        for status, url in reception_urls.items():
            print(f"   - {status} í˜ì´ì§€ ë¡œë”©...")
            soup = self._get_soup(url)
            if soup:
                receptions = self._parse_online_receptions(soup, status)
                all_results.extend(receptions)
            time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€

        print(f"\nì´ {len(all_results)}ê°œì˜ ì˜ˆì•½/ì ‘ìˆ˜ ì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        return all_results


# --- í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì½”ë“œ ---
if __name__ == "__main__":
    crawler = DDMReserveCrawler()
    crawled_data = crawler.crawl_all()

    output_filename = "ddm_reserve_test.json"
    result = {
        "data": crawled_data,
        "count": len(crawled_data),
        "updated_at": datetime.now().isoformat(),
        "crawl_info": {
            "source": "ë™ëŒ€ë¬¸êµ¬ ì˜ˆì•½í¬í„¸",
            "categories": ["ì „ì²´í”„ë¡œê·¸ë¨", "ì˜¨ë¼ì¸ì ‘ìˆ˜"],
            "status": ["ì ‘ìˆ˜ì˜ˆì •", "ì ‘ìˆ˜ì¤‘"],
        },
    }

    with open(output_filename, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ê²°ê³¼ê°€ '{output_filename}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # ìƒ˜í”Œ ì¶œë ¥
    if crawled_data:
        print("\nğŸ“‹ ìˆ˜ì§‘ëœ ë°ì´í„° ìƒ˜í”Œ (ìµœëŒ€ 5ê°œ):")
        print("-" * 50)
        for i, item in enumerate(crawled_data[:5], 1):
            print(f"\n[{i}] {item['title']}")
            print(f"    ìœ í˜•: {item['type']}")
            print(f"    ìƒíƒœ: {item['status']}")
            if "application_period" in item:
                print(f"    ì ‘ìˆ˜ê¸°ê°„: {item['application_period']}")
            if "department" in item:
                print(f"    ë‹´ë‹¹ë¶€ì„œ: {item['department']}")
