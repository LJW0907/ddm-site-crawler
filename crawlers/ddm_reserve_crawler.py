# crawlers/ddm_reserve_crawler.py

import time
from bs4 import BeautifulSoup
from datetime import datetime
import json
from urllib.parse import urljoin

# --- Selenium ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€ ---
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager


class DDMReserveCrawler:
    """ë™ëŒ€ë¬¸êµ¬ ì˜ˆì•½í¬í„¸ í¬ë¡¤ëŸ¬ (ì „ì²´í”„ë¡œê·¸ë¨ & ì˜¨ë¼ì¸ì ‘ìˆ˜ í†µí•©)"""

    def __init__(self):
        """í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”. requests ì„¸ì…˜ ëŒ€ì‹  í—¤ë” ì •ë³´ë§Œ ìœ ì§€"""
        self.base_url = "https://www.ddm.go.kr"
        # Seleniumì—ì„œë„ User-AgentëŠ” ì¤‘ìš”í•˜ë¯€ë¡œ í—¤ë” ì •ë³´ëŠ” ìœ ì§€í•©ë‹ˆë‹¤.
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
        }
        # requests.Session ê´€ë ¨ ì½”ë“œëŠ” ì œê±°í•©ë‹ˆë‹¤.
        # self.session = requests.Session()
        # self.session.headers.update(self.headers)

    def _get_soup(self, url, params=None):
        """
        [ìˆ˜ì •ëœ ë¶€ë¶„]
        Seleniumì„ ì‚¬ìš©í•´ BeautifulSoup ê°ì²´ë¥¼ ë°˜í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜.
        ì‹¤ì œ ë¸Œë¼ìš°ì €ë¥¼ êµ¬ë™í•˜ì—¬ ìë°”ìŠ¤í¬ë¦½íŠ¸ ë Œë”ë§ê³¼ ë´‡ ì°¨ë‹¨ì„ ìš°íšŒí•©ë‹ˆë‹¤.
        """
        driver = None  # driver ë³€ìˆ˜ ì´ˆê¸°í™”
        try:
            # Chrome ì˜µì…˜ ì„¤ì •
            options = Options()
            options.add_argument("--headless")  # ë¸Œë¼ìš°ì € ì°½ì„ ë„ìš°ì§€ ì•ŠëŠ” ì˜µì…˜
            options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage")
            options.add_argument(
                f"user-agent={self.headers['User-Agent']}"
            )  # User-Agent ì„¤ì •

            # ChromeDriver ìë™ ì„¤ì¹˜ ë° ì„œë¹„ìŠ¤ ì‹¤í–‰
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=options)

            # í˜ì´ì§€ ì ‘ì†
            driver.get(url)

            # í˜ì´ì§€ì˜ ëª¨ë“  ì½˜í…ì¸ (ìë°”ìŠ¤í¬ë¦½íŠ¸ í¬í•¨)ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ì ì‹œ ëŒ€ê¸°
            time.sleep(3)

            # ë Œë”ë§ëœ í˜ì´ì§€ì˜ HTML ì†ŒìŠ¤ë¥¼ ê°€ì ¸ì˜´
            html = driver.page_source

            # BeautifulSoup ê°ì²´ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
            return BeautifulSoup(html, "lxml")

        except Exception as e:
            print(f"Error fetching {url} with Selenium: {e}")
            return None
        finally:
            # ë“œë¼ì´ë²„ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆì„ ê²½ìš°ì—ë§Œ ì¢…ë£Œ
            if driver:
                driver.quit()

    # ==================================================================
    # ì•„ë˜ì˜ íŒŒì‹± ë° ì‹¤í–‰ ë¡œì§ì€ ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼í•©ë‹ˆë‹¤. (ìˆ˜ì • ì—†ìŒ)
    # ==================================================================

    def _parse_programs(self, soup, status):
        """'ì „ì²´í”„ë¡œê·¸ë¨' í˜ì´ì§€ì˜ ëª©ë¡ì„ íŒŒì‹±"""
        programs = []
        rows = soup.select("div.program.lecture tbody.text_center tr")

        if not rows:
            print(f"     -> ì „ì²´í”„ë¡œê·¸ë¨ {status}: ë°ì´í„° ì—†ìŒ")
            return programs

        for row in rows:
            cells = row.find_all("td")
            if len(cells) < 8:
                continue

            title_tag = cells[1].find("a")
            status_tag = cells[7].find("a")

            date_cell_text = cells[3].get_text(separator="|").strip()
            date_parts = [p.strip() for p in date_cell_text.split("|") if p.strip()]

            application_period = date_parts[0] if date_parts else ""
            education_period = date_parts[1] if len(date_parts) > 1 else ""

            detail_url = ""
            if title_tag and "href" in title_tag.attrs:
                detail_url = urljoin(self.base_url, title_tag["href"])
            elif status_tag and "onclick" in status_tag.attrs:
                onclick = status_tag["onclick"]
                if "location.href" in onclick:
                    url_match = onclick.split("'")[1] if "'" in onclick else ""
                    if url_match:
                        detail_url = urljoin(self.base_url, url_match)

            program = {
                "title": title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ",
                "location": cells[2].text.strip(),
                "application_period": application_period,
                "education_period": education_period,
                "education_time": cells[4].get_text(separator=" ", strip=True),
                "selection_method": cells[5].text.strip(),
                "capacity_status": cells[6].get_text(separator=" ", strip=True),
                "status": status,
                "button_text": status_tag.text.strip() if status_tag else "",
                "url": detail_url,
                "type": "ì „ì²´í”„ë¡œê·¸ë¨",
                "source": "ë™ëŒ€ë¬¸êµ¬ ì˜ˆì•½í¬í„¸",
                "crawled_at": datetime.now().isoformat(),
            }
            programs.append(program)

        print(f"     -> ì „ì²´í”„ë¡œê·¸ë¨ {status}: {len(programs)}ê°œ")
        return programs

    def _parse_online_receptions(self, soup, status):
        """'ì˜¨ë¼ì¸ì ‘ìˆ˜' í˜ì´ì§€ì˜ ëª©ë¡ì„ íŒŒì‹±"""
        receptions = []
        rows = soup.select("div.online_accept.list tbody.text_center tr")

        if not rows:
            print(f"     -> ì˜¨ë¼ì¸ì ‘ìˆ˜ {status}: ë°ì´í„° ì—†ìŒ")
            return receptions

        for row in rows:
            cells = row.find_all("td")
            if len(cells) < 8:
                continue

            title_tag = cells[1].find("a")
            status_tag = cells[7].find("a")

            detail_url = ""
            if title_tag and "href" in title_tag.attrs:
                detail_url = urljoin(self.base_url, title_tag["href"])
            elif status_tag and "onclick" in status_tag.attrs:
                onclick = status_tag["onclick"]
                if "location.href" in onclick:
                    url_match = onclick.split("'")[1] if "'" in onclick else ""
                    if url_match:
                        detail_url = urljoin(self.base_url, url_match)

            reception = {
                "title": title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ",
                "department": cells[2].text.strip(),
                "application_period": cells[3].get_text(separator="~", strip=True),
                "selection_method": cells[4].text.strip(),
                "capacity_status": cells[5]
                .get_text(separator="/", strip=True)
                .replace("\n", ""),
                "fee": cells[6].text.strip(),
                "status": status,
                "button_text": status_tag.text.strip() if status_tag else "",
                "url": detail_url,
                "type": "ì˜¨ë¼ì¸ì ‘ìˆ˜",
                "source": "ë™ëŒ€ë¬¸êµ¬ ì˜ˆì•½í¬í„¸",
                "crawled_at": datetime.now().isoformat(),
            }
            receptions.append(reception)

        print(f"     -> ì˜¨ë¼ì¸ì ‘ìˆ˜ {status}: {len(receptions)}ê°œ")
        return receptions

    def crawl_all(self):
        """ëª¨ë“  ì˜ˆì•½/ì ‘ìˆ˜ í”„ë¡œê·¸ë¨ì„ í¬ë¡¤ë§"""
        print("\n" + "=" * 50)
        print("   [ë™ëŒ€ë¬¸êµ¬ ì˜ˆì•½í¬í„¸] í¬ë¡¤ë§ ì‹œì‘")
        print("=" * 50 + "\n")

        all_results = []

        program_urls = {
            "ì ‘ìˆ˜ì˜ˆì •": "https://www.ddm.go.kr/reserve/selectDongdaemunUserCourseList.do?searchEduInstSe=&key=1529&searchEdcKey=&searchEdcRealm=&searchTime=%EC%A0%91%EC%88%98%EA%B8%B0%EA%B0%84&timeBgnde=&timeEndde=&receptionStts=TBCCPT&searchCnd=SJ&searchKrwd=",
            "ì ‘ìˆ˜ì¤‘": "https://www.ddm.go.kr/reserve/selectDongdaemunUserCourseList.do?searchEduInstSe=&key=1529&searchEdcKey=&searchEdcRealm=&searchTime=%EC%A0%91%EC%88%98%EA%B8%B0%EA%B0%84&timeBgnde=&timeEndde=&receptionStts=ACCPT&searchCnd=SJ&searchKrwd=",
        }

        print("1. [ì „ì²´í”„ë¡œê·¸ë¨] í¬ë¡¤ë§")
        for status, url in program_urls.items():
            print(f"   - {status} í˜ì´ì§€ ë¡œë”©...")
            soup = self._get_soup(url)
            if soup:
                programs = self._parse_programs(soup, status)
                all_results.extend(programs)
            # Seleniumì€ ìì²´ì ìœ¼ë¡œ ë¡œë”© ì‹œê°„ì´ ìˆìœ¼ë¯€ë¡œ time.sleep()ì„ ì¤„ì´ê±°ë‚˜ ì œê±°í•´ë„ ë©ë‹ˆë‹¤.
            # time.sleep(1)

        reception_urls = {
            "ì ‘ìˆ˜ì˜ˆì •": "https://www.ddm.go.kr/reserve/selectUserOnlineReceptionList.do?key=3133&searchCnd=TBCCPT",
            "ì ‘ìˆ˜ì¤‘": "https://www.ddm.go.kr/reserve/selectUserOnlineReceptionList.do?key=3133&searchCnd=ACCPT",
        }

        print("\n2. [ì˜¨ë¼ì¸ì ‘ìˆ˜] í¬ë¡¤ë§")
        for status, url in reception_urls.items():
            print(f"   - {status} í˜ì´ì§€ ë¡œë”©...")
            soup = self._get_soup(url)
            if soup:
                receptions = self._parse_online_receptions(soup, status)
                all_results.extend(receptions)
            # time.sleep(1)

        print(f"\nì´ {len(all_results)}ê°œì˜ ì˜ˆì•½/ì ‘ìˆ˜ ì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        return all_results


# --- í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì½”ë“œ ---
if __name__ == "__main__":
    crawler = DDMReserveCrawler()
    crawled_data = crawler.crawl_all()

    output_filename = "ddm_reserve_test.json"
    result = {
        "data": crawled_data,
        "count": len(crawled_data),
        "updated_at": datetime.now().isoformat(),
        "crawl_info": {
            "source": "ë™ëŒ€ë¬¸êµ¬ ì˜ˆì•½í¬í„¸",
            "categories": ["ì „ì²´í”„ë¡œê·¸ë¨", "ì˜¨ë¼ì¸ì ‘ìˆ˜"],
            "status": ["ì ‘ìˆ˜ì˜ˆì •", "ì ‘ìˆ˜ì¤‘"],
        },
    }

    with open(output_filename, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ê²°ê³¼ê°€ '{output_filename}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    if crawled_data:
        print("\nğŸ“‹ ìˆ˜ì§‘ëœ ë°ì´í„° ìƒ˜í”Œ (ìµœëŒ€ 5ê°œ):")
        print("-" * 50)
        for i, item in enumerate(crawled_data[:5], 1):
            print(f"\n[{i}] {item['title']}")
            print(f"    ìœ í˜•: {item['type']}")
            print(f"    ìƒíƒœ: {item['status']}")
            if "application_period" in item:
                print(f"    ì ‘ìˆ˜ê¸°ê°„: {item['application_period']}")
            if "department" in item:
                print(f"    ë‹´ë‹¹ë¶€ì„œ: {item['department']}")
