# crawlers/ddm_news_crawler.py

import time
import re
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from datetime import datetime
from dateutil.relativedelta import relativedelta
import json
import os


def crawl_ddm_news():
    """ë™ëŒ€ë¬¸êµ¬ì²­ êµìœ¡ì†Œì‹ ê²Œì‹œíŒ í¬ë¡¤ë§"""
    BASE_URL = "https://www.ddm.go.kr/www/"
    URL_TEMPLATE = "https://www.ddm.go.kr/www/selectBbsNttList.do?key=575&bbsNo=38&searchCtgry=%ea%b5%90%ec%9c%a1&pageIndex={page}"

    # GitHub Actions í™˜ê²½ ì²´í¬
    if os.environ.get("GITHUB_ACTIONS"):
        options = webdriver.ChromeOptions()
        options.add_argument("--headless")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--window-size=1920x1080")
        driver = webdriver.Chrome(options=options)
    else:
        # ë¡œì»¬ í™˜ê²½ìš© ì„¤ì •
        from webdriver_manager.chrome import ChromeDriverManager

        service = Service(ChromeDriverManager().install())
        options = webdriver.ChromeOptions()
        options.add_argument("--headless")
        driver = webdriver.Chrome(service=service, options=options)

    # í¬ë¡¤ë§ ë²”ìœ„ ì„¤ì •: ì§€ë‚œë‹¬ 1ì¼
    today = datetime.now().date()
    first_day_of_current_month = today.replace(day=1)
    threshold_date = first_day_of_current_month - relativedelta(months=1)

    print("\n" + "=" * 50)
    print("   [ë™ëŒ€ë¬¸êµ¬ì²­ êµìœ¡ì†Œì‹] í¬ë¡¤ë§ ì‹œì‘")
    print(f"   ë°ì´í„° ìˆ˜ì§‘ ë²”ìœ„: {threshold_date} ì´í›„ ê²Œì‹œë¬¼")
    print("=" * 50 + "\n")

    results = []
    page_index = 1
    stop_crawling = False
    consecutive_errors = 0

    try:
        while not stop_crawling and consecutive_errors < 3:
            target_url = URL_TEMPLATE.format(page=page_index)
            print(f"í˜ì´ì§€ {page_index} ë¡œë”© ì¤‘...")

            try:
                driver.get(target_url)

                # tbodyê°€ ë¡œë“œë  ë•Œê¹Œì§€ ìµœëŒ€ 10ì´ˆ ëŒ€ê¸°
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located(
                        (By.CSS_SELECTOR, "tbody.text_center")
                    )
                )

                html = driver.page_source
                soup = BeautifulSoup(html, "lxml")

                notice_list = soup.select("tbody.text_center tr")

                if not notice_list:
                    print("ê²Œì‹œë¬¼ì´ ë” ì´ìƒ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break

                page_items = 0
                for notice in notice_list:
                    # ê³µì§€ì‚¬í•­(img alt="ê³µì§€")ì€ ê±´ë„ˆë›°ê¸°
                    if notice.find("img", alt="ê³µì§€"):
                        continue

                    cells = notice.find_all("td")

                    # í…Œì´ë¸” êµ¬ì¡° í™•ì¸: ë²ˆí˜¸(0), ì œëª©(1), ë‹´ë‹¹ë¶€ì„œ(2), ì‘ì„±ì¼(3), ì²¨ë¶€(4)
                    if len(cells) < 4:
                        continue

                    try:
                        # ë‚ ì§œ ì¶”ì¶œ ë° ê²€ì¦ - ìˆ˜ì •ëœ ë¶€ë¶„
                        date_cell = cells[3]
                        date_text = date_cell.text.strip()

                        # "ì‘ì„±ì¼" í…ìŠ¤íŠ¸ ì œê±° ë° ê³µë°± ì •ë¦¬
                        date_text = date_text.replace("ì‘ì„±ì¼", "").strip()
                        # ì—¬ëŸ¬ ì¤„ ê³µë°± ì œê±°
                        date_text = re.sub(r"\s+", " ", date_text).strip()

                        # ë‚ ì§œ í˜•ì‹ ë§¤ì¹­ (YYYY-MM-DD)
                        date_match = re.search(r"(\d{4}-\d{2}-\d{2})", date_text)
                        if not date_match:
                            print(f"ë‚ ì§œ í˜•ì‹ ì¸ì‹ ì‹¤íŒ¨: {repr(date_text)}")
                            continue

                        date_str = date_match.group(1)
                        post_date = datetime.strptime(date_str, "%Y-%m-%d").date()

                        # ë‚ ì§œê°€ ê¸°ì¤€ì¼ ì´ì „ì´ë©´ í¬ë¡¤ë§ ì¤‘ë‹¨
                        if post_date < threshold_date:
                            stop_crawling = True
                            print(
                                f"ê¸°ì¤€ì¼({threshold_date}) ì´ì „ ê²Œì‹œë¬¼ ë°œê²¬. í¬ë¡¤ë§ ì¤‘ë‹¨."
                            )
                            break

                        # ì œëª© ë° URL ì¶”ì¶œ
                        title_cell = cells[1]
                        title_tag = title_cell.find("a")
                        if not title_tag:
                            continue

                        title = title_tag.text.strip()

                        # onclick ì†ì„±ì—ì„œ nttNo ì¶”ì¶œí•˜ì—¬ ì‹¤ì œ URL ìƒì„±
                        onclick = title_tag.get("onclick", "")
                        href = title_tag.get("href", "")

                        if onclick and "selectBbsNttView" in onclick:
                            # onclickì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
                            ntt_no_match = re.search(
                                r'nttNo["\s]*[:=]["\s]*(\d+)', onclick
                            )
                            if ntt_no_match:
                                ntt_no = ntt_no_match.group(1)
                                absolute_url = f"https://www.ddm.go.kr/www/selectBbsNttView.do?key=575&bbsNo=38&nttNo={ntt_no}"
                            else:
                                # onclick íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ì²˜ë¦¬
                                absolute_url = (
                                    BASE_URL + "selectBbsNttList.do?key=575&bbsNo=38"
                                )
                        elif href and href != "#" and href != "javascript:void(0);":
                            # hrefê°€ ìœ íš¨í•œ ê²½ìš°
                            if href.startswith("http"):
                                absolute_url = href
                            else:
                                absolute_url = BASE_URL + href.lstrip("/")
                        else:
                            # URL ì¶”ì¶œ ì‹¤íŒ¨
                            print(f"URL ì¶”ì¶œ ì‹¤íŒ¨: {title}")
                            absolute_url = (
                                BASE_URL + "selectBbsNttList.do?key=575&bbsNo=38"
                            )

                        # ë‹´ë‹¹ë¶€ì„œ ì¶”ì¶œ
                        dept_cell = cells[2]
                        department = dept_cell.text.strip()

                        # ì²¨ë¶€íŒŒì¼ ì—¬ë¶€ í™•ì¸
                        has_attachment = False
                        if len(cells) > 4:
                            attachment_cell = cells[4]
                            if (
                                attachment_cell.find("img")
                                or "ì²¨ë¶€" in attachment_cell.text
                            ):
                                has_attachment = True

                        results.append(
                            {
                                "title": title,
                                "date": date_str,
                                "department": department,
                                "url": absolute_url,
                                "has_attachment": has_attachment,
                                "type": "êµìœ¡ì†Œì‹",
                                "source": "ë™ëŒ€ë¬¸êµ¬ì²­",
                                "crawled_at": datetime.now().isoformat(),
                            }
                        )
                        page_items += 1

                    except Exception as e:
                        print(f"í•­ëª© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue

                print(f"  - í˜ì´ì§€ {page_index}: {page_items}ê°œ í•­ëª© ìˆ˜ì§‘")

                if not stop_crawling and page_items > 0:
                    page_index += 1
                    consecutive_errors = 0
                    time.sleep(1)  # í˜ì´ì§€ ê°„ 1ì´ˆ ëŒ€ê¸°
                elif page_items == 0:
                    # ë¹ˆ í˜ì´ì§€ì¸ ê²½ìš° ì¢…ë£Œ
                    print("ë” ì´ìƒ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                    break

            except Exception as e:
                print(f"í˜ì´ì§€ {page_index} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                consecutive_errors += 1
                if consecutive_errors >= 3:
                    print("ì—°ì† ì˜¤ë¥˜ ë°œìƒìœ¼ë¡œ í¬ë¡¤ë§ ì¤‘ë‹¨")
                    break
                time.sleep(2)  # ì˜¤ë¥˜ ë°œìƒì‹œ 2ì´ˆ ëŒ€ê¸°

    except Exception as e:
        print(f"í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
    finally:
        driver.quit()

    print(f"\nì´ {len(results)}ê°œì˜ êµìœ¡ì†Œì‹ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
    return results


# í…ŒìŠ¤íŠ¸ìš© ì‹¤í–‰
if __name__ == "__main__":
    crawled_data = crawl_ddm_news()

    output_filename = "ddm_news_test.json"
    result = {
        "data": crawled_data,
        "count": len(crawled_data),
        "updated_at": datetime.now().isoformat(),
        "crawl_info": {
            "source": "ë™ëŒ€ë¬¸êµ¬ì²­ êµìœ¡ì†Œì‹",
            "url": "https://www.ddm.go.kr/www/selectBbsNttList.do?key=575&bbsNo=38&searchCtgry=êµìœ¡",
        },
    }

    with open(output_filename, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"âœ… ê²°ê³¼ê°€ '{output_filename}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # ìƒ˜í”Œ ì¶œë ¥
    if crawled_data:
        print("\nğŸ“‹ ìˆ˜ì§‘ëœ ë°ì´í„° ìƒ˜í”Œ (ìµœëŒ€ 5ê°œ):")
        print("-" * 50)
        for i, item in enumerate(crawled_data[:5], 1):
            print(f"\n[{i}] {item['title']}")
            print(f"    ë‚ ì§œ: {item['date']}")
            print(f"    ë¶€ì„œ: {item['department']}")
            print(f"    ì²¨ë¶€: {'ìˆìŒ' if item.get('has_attachment') else 'ì—†ìŒ'}")
            print(f"    URL: {item['url'][:60]}...")
