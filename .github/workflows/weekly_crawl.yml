name: Bi-Weekly Program Crawler

on:
  schedule:
    - cron: "0 15 * * 2,5"
  workflow_dispatch:

jobs:
  crawl:
    runs-on: ubuntu-latest

    steps:
      - name: 코드 체크아웃
        uses: actions/checkout@v4

      - name: Python 설정
        uses: actions/setup-python@v5
        with:
          python-version: "3.9"

      - name: Chrome 브라우저 설정
        uses: browser-actions/setup-chrome@v1
        with:
          chrome-version: stable

      - name: 파이썬 패키지 설치
        run: |
          pip install selenium beautifulsoup4 boto3 lxml

      - name: 크롤링 실행
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: ap-southeast-2
          GITHUB_ACTIONS: true
        run: python main_crawler.py

      - name: 결과 업로드
        uses: actions/upload-artifact@v4
        with:
          name: crawling-results
          path: |
            *.json
          retention-days: 30
